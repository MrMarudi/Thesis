{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>__Important notice:   \n",
    "place make sure to install the scikit-learn customized package before running this notebook to activated the ordinal models.  \n",
    "Installation guidance are in the README file__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-62-13f572485711>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-62-13f572485711>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://www.mdpi.com/1099-4300/22/8/871\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://www.mdpi.com/1099-4300/22/8/871"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Decision-Trees Based Approach for Ordinal Classification Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages  \n",
    "\n",
    "from progressbar import ProgressBar\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import operator\n",
    "import sklearn.metrics as  metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import scipy.stats as stats\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from scipy import stats\n",
    "#from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ignore sklearn errors \n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This function run our experiments:\n",
    "Inputs:  \n",
    "\n",
    "- list of models, each model is a tuple (index, model_name, model_object untrained)\n",
    "- data sets as a dataframe \n",
    "- rnd and spp – hyper parameters to create kfold object \n",
    "  \n",
    "Logic:  \n",
    "- Initialize lists to save models scores both for model total score and per class \n",
    "- Run each model on the dataset and calculate all predefine metrics \n",
    "    - Initialize lists to save models scores both for model total score and per class per model \n",
    "    - Initialize kfold object, make sure each model been calculated on the same splits (5 folds)\n",
    "    - Loop that run per kfold split:\n",
    "        - Train the model\n",
    "        - Predict on test set \n",
    "        - Calculate all metrics \n",
    "    - Save for each model family their counterpart\n",
    "    - Check if compression should be made, if yes calculate p_value else put default values \n",
    "    - Average all fold results to one score. \n",
    "- Create 2 dataframes , one model level second class level and return them \n",
    "\n",
    "Output:\n",
    "- 2 Dataframe with model score. First contain average score second with score std \n",
    "- 2 Dataframe with model score per class. First contain average score second with score std\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run each model\n",
    "def calc_results(models,data,kfold,rnd = 1,spp = 20):\n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1] #set label columns \n",
    "    \n",
    "    #Initialize lists to save models scores \n",
    "    pred_df = []\n",
    "    pred__proba_df = []\n",
    "    \n",
    "    names = []\n",
    "\n",
    "    model_lvl_accuracy_score_avg = []\n",
    "    model_lvl_accuracy_score_std = []\n",
    "\n",
    "    model_lvl_f1_weighted_score_avg = []\n",
    "    model_lvl_f1_weighted_score_std = []\n",
    "\n",
    "    model_lvl_recall_score_avg = []\n",
    "    model_lvl_recall_score_std = []\n",
    "\n",
    "    model_lvl_precision_score_avg =[]\n",
    "    model_lvl_precision_score_std =[]\n",
    "\n",
    "    model_lvl_MSE_score_avg = []\n",
    "    model_lvl_MSE_score_std = []\n",
    "\n",
    "    model_lvl_roc_auc_avg = []\n",
    "    model_lvl_roc_auc_std = []\n",
    "\n",
    "    model_lvl_kendall_tau = []\n",
    "    model_lvl_kendall_p_value = []\n",
    "\n",
    "    model_lvl_ttest_p_value_roc = []\n",
    "    model_lvl_ttest_p_value_auc = []\n",
    "    model_lvl_ttest_p_value_f1 = []\n",
    "    model_lvl_ttest_p_value_recall = []\n",
    "    model_lvl_ttest_p_value_precision = []\n",
    "    model_lvl_ttest_p_value_MSE = []\n",
    "    model_lvl_ttest_p_value_kendall = []\n",
    "    \n",
    "    ttest_champ_roc= []\n",
    "    ttest_champ_auc= []\n",
    "    ttest_champ_f1= []\n",
    "    ttest_champ_recall= []\n",
    "    ttest_champ_precision= []\n",
    "    ttest_champ_MSE= []\n",
    "    ttest_champ_kendall= []\n",
    "    \n",
    "    curr_champ = '989'\n",
    "    \n",
    "    #class\n",
    "    class_lvl_MSE_avg = []\n",
    "    class_lvl_MSE_std = []\n",
    "    \n",
    "    class_lvl_EV_avg = []\n",
    "    class_lvl_EV_std = []\n",
    "    \n",
    "    class_lvl_roc_auc_avg = []\n",
    "    class_lvl_roc_auc_std = []\n",
    "\n",
    "    class_lvl_f1_score_avg = []\n",
    "    class_lvl_f1_score_std = []\n",
    "\n",
    "    class_lvl_recall_score_avg = []\n",
    "    class_lvl_recall_score_std = []\n",
    "\n",
    "    class_lvl_precision_score_avg = []\n",
    "    class_lvl_precision_score_std = []\n",
    "    \n",
    "    \n",
    "    print('total models : ', len(models))\n",
    "    pbar = ProgressBar()\n",
    "    \n",
    "    # Loop that run each model on the dataset \n",
    "    for num,name,model in pbar(models):\n",
    "    \n",
    "        #metrics per model \n",
    "        accuracy_results = []\n",
    "        f1_weighted_results = []\n",
    "        recall_results = []\n",
    "        precision_results =[]\n",
    "        MSE = []\n",
    "        roc_auc = []\n",
    "        kendall_tau = []\n",
    "        kendall_p_value = []\n",
    "        \n",
    "        #metric per class\n",
    "        class_MSE_results = [] \n",
    "        class_EV_results = [] \n",
    "        class_roc_auc = []\n",
    "        class_f1_results = []\n",
    "        class_recall_results = []\n",
    "        class_precision_results = [] \n",
    "\n",
    "\n",
    "        #Initialize kfold object\n",
    "        kfold = StratifiedKFold(n_splits=spp,shuffle = True,random_state = rnd)\n",
    "        for train_index, test_index in kfold.split(X,Y):\n",
    "\n",
    "            # Fix for the ordinal binary model approach. \n",
    "            # This model can’t be retrained after trained once so we need to Initialize new model after each fold \n",
    "            #(code can be found : https://github.com/JOEYGLASSER/CLEVELAND-HEARTS-ANALYSIS)\n",
    "            if name=='OrdinalModel':\n",
    "                model =  OrdinalModel()   \n",
    "\n",
    "            #split for train and test based on kfold index\n",
    "            X_train, X_test = X.loc[train_index,:], X.loc[test_index,:]\n",
    "            y_train, y_test = Y.loc[train_index], Y.loc[test_index]\n",
    "            \n",
    "            #train the model\n",
    "            model.fit(X_train,y_train)\n",
    "            \n",
    "            #predict vector \n",
    "            pred = model.predict(X_test)\n",
    "            \n",
    "            #Calculate each metric per fold averaged by weighted, \n",
    "            #this option take into consideration the classes distribution \n",
    "            \n",
    "            accuracy_results.append(metrics.accuracy_score(y_test,pred))\n",
    "            f1_weighted_results.append(metrics.f1_score(y_test,pred,average = 'weighted'))\n",
    "            recall_results.append(metrics.recall_score(y_test,pred,average = 'weighted'))\n",
    "            precision_results.append(metrics.precision_score(y_test,pred,average = 'weighted'))\n",
    "            MSE.append(metrics.mean_squared_error(y_test,pred))\n",
    "            \n",
    "            \n",
    "            #Fix: when predict vector don’t contain all classes in the data set \n",
    "            #Add 1 line with the missing class, \n",
    "            #Assumption: since there are many samples and we calculate each metrics based on the class distribution , \n",
    "            #add 1 fake line with the missing class wont shift our scores \n",
    "            \n",
    "            Y_uniqe = list(pd.unique(Y))\n",
    "            Y_uniqe.sort()\n",
    "            y_test_uniqe = list(pd.unique(y_test))\n",
    "            pred_uniqe = list(pd.unique(pred))\n",
    "            y_test_pd = pd.get_dummies(y_test)\n",
    "            pred_pd = pd.get_dummies(pred)\n",
    "            y_test_pd = y_test_pd.reset_index(drop = True)\n",
    "            pred_pd = pred_pd.reset_index(drop = True)\n",
    "            for label in Y_uniqe:\n",
    "                if label not in pred_uniqe:\n",
    "                    pred_pd[label] = 0\n",
    "\n",
    "                if label not in y_test_uniqe:\n",
    "            \n",
    "                    y_test_pd[label] = 0 \n",
    "                    extra_row = [0]*y_test_pd.shape[1]\n",
    "                    extra_row[-1] = 1\n",
    "                    y_test_pd.loc[len(y_test_pd)+2]= extra_row\n",
    "                    pred_pd.loc[len(y_test_pd)-1]= [0]*pred_pd.shape[1]\n",
    "                    y_test_pd = y_test_pd.reset_index(drop = True)\n",
    "                    pred_pd = pred_pd.reset_index(drop = True)\n",
    "                    \n",
    "            cols = list(y_test_pd.columns)\n",
    "\n",
    "            cols.sort()\n",
    "            y_test_pd = y_test_pd[cols]\n",
    "            pred_pd = pred_pd[cols]\n",
    "\n",
    " \n",
    "            roc_auc.append(metrics.roc_auc_score(y_test_pd,pred_pd,average = 'weighted' , multi_class = 'ovr'))\n",
    "            tau ,p_value =  stats.kendalltau(y_test,pred)\n",
    "            kendall_tau.append(tau)\n",
    "            kendall_p_value.append(p_value) \n",
    "            \n",
    "            #Calculate each metric per fold per class          \n",
    "            class_roc_auc.append(metrics.roc_auc_score(y_test_pd,pred_pd,average = None , multi_class = 'ovr'))\n",
    "                  \n",
    "            y_test_fix = pd.Series(y_test_pd.columns[np.where(y_test_pd!=0)[1]])\n",
    "            pred_pd_fix = pd.Series(pred_pd.columns[np.where(pred_pd!=0)[1]])\n",
    "\n",
    "            class_f1_results.append(metrics.f1_score(y_test_fix,pred_pd_fix,average = None))\n",
    "            class_recall_results.append(metrics.recall_score(y_test_fix,pred_pd_fix,average = None))\n",
    "            class_precision_results.append(metrics.precision_score(y_test_fix,pred_pd_fix,average = None))\n",
    "\n",
    "            #Calculate MSE and EV per class\n",
    "            pred_pd2 =pred\n",
    "            y_test_pd2 =y_test.to_numpy(dtype='int64')\n",
    "            MSE_res_fold = []\n",
    "            EV_res_fold = []\n",
    "            for label in Y_uniqe:\n",
    "                pred_pd2_label = pred_pd2[np.argwhere(pred_pd2==label).T][0]\n",
    "                y_test_pd2_label = y_test_pd2[np.argwhere(pred_pd2==label).T][0]\n",
    "                if pred_pd2_label.shape[0] > 0: \n",
    "                    MSE_res_fold.append(metrics.mean_squared_error(pred_pd2_label,y_test_pd2_label))\n",
    "                    #MSE_res_fold.append(metrics.mean_absolute_error(pred_pd2_label,y_test_pd2_label))\n",
    "                    EV_res_fold.append(y_test_pd2_label.mean())\n",
    "                else:\n",
    "                    MSE_res_fold.append(0)\n",
    "                    EV_res_fold.append(0)\n",
    "                    \n",
    "            class_MSE_results.append(MSE_res_fold)\n",
    "            class_EV_results.append(EV_res_fold)\n",
    "            \n",
    "            #end of k-fold loop\n",
    "\n",
    "\n",
    "        #Save for each model’s families their counterpart, that should be compared to   \n",
    "        if len(num) == 1:\n",
    "            ttest_champ_roc= roc_auc \n",
    "            ttest_champ_auc= accuracy_results\n",
    "            ttest_champ_f1= f1_weighted_results\n",
    "            ttest_champ_recall= recall_results\n",
    "            ttest_champ_precision = precision_results\n",
    "            ttest_champ_MSE = MSE\n",
    "            ttest_champ_kendall   = kendall_tau\n",
    "            curr_champ = num\n",
    "            \n",
    "        # Check if compression should be made\n",
    "        # If yes: calculate p_value per metric \n",
    "        # If no : put default values    \n",
    "        if curr_champ in num and len(num) != 1:\n",
    "\n",
    "            model_lvl_ttest_p_value_roc.append(stats.ttest_rel(ttest_champ_roc,roc_auc)[1])\n",
    "            model_lvl_ttest_p_value_auc.append(stats.ttest_rel(ttest_champ_auc,accuracy_results)[1])\n",
    "            model_lvl_ttest_p_value_f1.append(stats.ttest_rel(ttest_champ_f1,f1_weighted_results)[1])\n",
    "            model_lvl_ttest_p_value_recall.append(stats.ttest_rel(ttest_champ_recall,recall_results)[1])\n",
    "            model_lvl_ttest_p_value_precision.append(stats.ttest_rel(ttest_champ_precision,precision_results)[1])\n",
    "            model_lvl_ttest_p_value_MSE.append(stats.ttest_rel(ttest_champ_MSE,MSE)[1])     \n",
    "            model_lvl_ttest_p_value_kendall.append(stats.ttest_rel(ttest_champ_kendall,kendall_tau)[1])         \n",
    "          \n",
    "        else:\n",
    "\n",
    "            model_lvl_ttest_p_value_roc.append(9)\n",
    "            model_lvl_ttest_p_value_auc.append(9)\n",
    "            model_lvl_ttest_p_value_f1.append(9)\n",
    "            model_lvl_ttest_p_value_recall.append(9)\n",
    "            model_lvl_ttest_p_value_precision.append(9)\n",
    "            model_lvl_ttest_p_value_MSE.append(9)  \n",
    "            model_lvl_ttest_p_value_kendall.append(9) \n",
    "\n",
    "        #Average all fold results to one score. \n",
    "        pred_df.append([name,pred])\n",
    "        \n",
    "        model_lvl_accuracy_score_avg.append(np.average(accuracy_results))\n",
    "        model_lvl_accuracy_score_std.append(np.std(accuracy_results))\n",
    "\n",
    "        model_lvl_f1_weighted_score_avg.append(np.average(f1_weighted_results))\n",
    "        model_lvl_f1_weighted_score_std.append(np.std(f1_weighted_results))\n",
    "\n",
    "        model_lvl_recall_score_avg.append(np.average(recall_results))\n",
    "        model_lvl_recall_score_std.append(np.std(recall_results))\n",
    "\n",
    "        model_lvl_precision_score_avg.append(np.average(precision_results))\n",
    "        model_lvl_precision_score_std.append(np.std(precision_results))\n",
    "\n",
    "        model_lvl_MSE_score_avg.append(np.average(MSE))\n",
    "        model_lvl_MSE_score_std.append(np.std(MSE))\n",
    "\n",
    "        model_lvl_roc_auc_avg.append(np.average(roc_auc))\n",
    "        model_lvl_roc_auc_std.append(np.std(roc_auc))\n",
    "        \n",
    "        model_lvl_kendall_tau.append(np.average(kendall_tau))\n",
    "        model_lvl_kendall_p_value.append(np.average(kendall_p_value)) \n",
    "\n",
    "        class_lvl_MSE_avg.append(np.average(np.array(class_MSE_results) ,axis=0))\n",
    "        class_lvl_MSE_std.append(np.std(np.array(class_MSE_results) ,axis=0))\n",
    "\n",
    "        class_lvl_EV_avg.append(np.average(np.array(class_EV_results) ,axis=0))\n",
    "        class_lvl_EV_std.append(np.std(np.array(class_EV_results) ,axis=0))\n",
    "        \n",
    "        class_lvl_roc_auc_avg.append(np.average(np.array(class_roc_auc) ,axis=0))\n",
    "        class_lvl_roc_auc_std.append(np.std(np.array(class_roc_auc) ,axis=0))\n",
    "        \n",
    "        class_lvl_f1_score_avg.append(np.average(np.array(class_f1_results) ,axis=0))\n",
    "        class_lvl_f1_score_std.append(np.std(np.array(class_f1_results) ,axis=0))\n",
    "\n",
    "        class_lvl_recall_score_avg.append(np.average(np.array(class_recall_results) ,axis=0))\n",
    "        class_lvl_recall_score_std.append(np.std(np.array(class_recall_results) ,axis=0))\n",
    "\n",
    "        class_lvl_precision_score_avg.append(np.average(np.array(class_precision_results) ,axis=0))\n",
    "        class_lvl_precision_score_std.append(np.std(np.array(class_precision_results) ,axis=0))\n",
    "        \n",
    "        names.append(name)\n",
    "        indexs = list(range(1,len(names)+1))\n",
    "\n",
    "    #Create 2 dataframes , one model level second class level and return them    \n",
    "    data_avg = pd.DataFrame({'accuracy_score_avg':model_lvl_accuracy_score_avg ,\n",
    "                            'f1_weighted_score_avg' :model_lvl_f1_weighted_score_avg ,\n",
    "                            \"recall_score_avg\" :model_lvl_recall_score_avg ,\n",
    "                            \"precision_score_avg\" :model_lvl_precision_score_avg ,\n",
    "                            \"MSE_score_avg\" :model_lvl_MSE_score_avg,\n",
    "                            'roc_auc_score_avg' : model_lvl_roc_auc_avg,\n",
    "                            'kendall_tau' : model_lvl_kendall_tau , \n",
    "                            'ttest_p_value_roc' : model_lvl_ttest_p_value_roc ,               \n",
    "                            'ttest_p_value_auc':model_lvl_ttest_p_value_auc,\n",
    "                            'ttest_p_value_f1':model_lvl_ttest_p_value_f1,\n",
    "                            'ttest_p_value_recall':model_lvl_ttest_p_value_recall,\n",
    "                            'ttest_p_value_percision':model_lvl_ttest_p_value_precision,\n",
    "                            'ttest_p_value_kendall'  :model_lvl_ttest_p_value_kendall , \n",
    "                            'ttest_p_value_MSE'   :model_lvl_ttest_p_value_MSE,\n",
    "                             'indexs' : indexs\n",
    "                } ,index=names)\n",
    "\n",
    "    data_std = pd.DataFrame({'accuracy_score_std':model_lvl_accuracy_score_std ,\n",
    "               'f1_weighted_score_std' :model_lvl_f1_weighted_score_std ,\n",
    "               \"recall_score_std\" :model_lvl_recall_score_std ,\n",
    "               \"precision_score_std\" :model_lvl_precision_score_std ,\n",
    "               \"MSE_score_std\" :model_lvl_MSE_score_std,\n",
    "                'roc_auc_score_std' : model_lvl_roc_auc_std,\n",
    "                'kendall_p_value' : model_lvl_kendall_p_value\n",
    "                            } ,index=names)\n",
    "\n",
    "    #class level avg\n",
    "    class_lvl_f1_score_avg = pd.DataFrame(class_lvl_f1_score_avg,index=names)\n",
    "    class_lvl_f1_score_avg[\"metric\"] = 'class_lvl_f1_score_avg'\n",
    "    class_lvl_recall_score_avg = pd.DataFrame(class_lvl_recall_score_avg,index=names)\n",
    "    class_lvl_recall_score_avg[\"metric\"] = 'class_lvl_recall_score_avg'\n",
    "    class_lvl_precision_score_avg = pd.DataFrame(class_lvl_precision_score_avg,index=names)\n",
    "    class_lvl_precision_score_avg[\"metric\"] = 'class_lvl_precision_score_avg'\n",
    "    class_lvl_roc_auc_avg = pd.DataFrame(class_lvl_roc_auc_avg,index=names)\n",
    "    class_lvl_roc_auc_avg[\"metric\"] = 'class_lvl_roc_auc_avg'      \n",
    "    class_lvl_MSE_avg = pd.DataFrame(class_lvl_MSE_avg,index=names)\n",
    "    class_lvl_MSE_avg[\"metric\"] = 'class_lvl_MSE_avg'  \n",
    "    class_lvl_EV_avg = pd.DataFrame(class_lvl_EV_avg,index=names)\n",
    "    class_lvl_EV_avg[\"metric\"] = 'class_lvl_EV_avg'  \n",
    "\n",
    "    data__class_avg = pd.concat([class_lvl_f1_score_avg,\n",
    "                                 class_lvl_recall_score_avg,\n",
    "                                 class_lvl_precision_score_avg,\n",
    "                                 class_lvl_roc_auc_avg,\n",
    "                                class_lvl_MSE_avg,\n",
    "                                class_lvl_EV_avg])\n",
    "    \n",
    "    cols = data__class_avg.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    data__class_avg = data__class_avg[cols]\n",
    "    \n",
    "    #class level std\n",
    "    class_lvl_f1_score_std = pd.DataFrame(class_lvl_f1_score_std,index=names)\n",
    "    class_lvl_f1_score_std[\"metric\"] = 'class_lvl_f1_score_std'\n",
    "    class_lvl_recall_score_std = pd.DataFrame(class_lvl_recall_score_std,index=names)\n",
    "    class_lvl_recall_score_std[\"metric\"] = 'class_lvl_recall_score_std'\n",
    "    class_lvl_precision_score_std = pd.DataFrame(class_lvl_precision_score_std,index=names)\n",
    "    class_lvl_precision_score_std[\"metric\"] = 'class_lvl_precision_score_std'\n",
    "    class_lvl_roc_auc_std = pd.DataFrame(class_lvl_roc_auc_std,index=names)\n",
    "    class_lvl_roc_auc_std[\"metric\"] = 'class_lvl_roc_auc_std'\n",
    "    class_lvl_MSE_std = pd.DataFrame(class_lvl_MSE_std,index=names)\n",
    "    class_lvl_MSE_std[\"metric\"] = 'class_lvl_MSE_std'\n",
    "    class_lvl_EV_std = pd.DataFrame(class_lvl_EV_std,index=names)\n",
    "    class_lvl_EV_std[\"metric\"] = 'class_lvl_EV_std'    \n",
    "    \n",
    "    data__class_std = pd.concat([class_lvl_f1_score_std,\n",
    "                                 class_lvl_recall_score_std,\n",
    "                                 class_lvl_precision_score_std,\n",
    "                                 class_lvl_roc_auc_std,\n",
    "                                 class_lvl_MSE_std,\n",
    "                                 class_lvl_EV_std])\n",
    "    \n",
    "    cols = data__class_std.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    data__class_std = data__class_std[cols]\n",
    "    for i in range(3,8):\n",
    "        if i not in list(pd.unique(Y)):\n",
    "            data__class_avg[i] = np.nan\n",
    "            data__class_std [i] = np.nan\n",
    "\n",
    "    return data_avg.T,data_std.T,data__class_avg,data__class_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization the normalization factor\n",
    "\n",
    "The next sets of functions try to find the best normalization factor per data sets by running a grid search.  \n",
    "Each function built for different algorithms families :\n",
    "-\tDecision trees\n",
    "-\tRandom forests \n",
    "-\tAdaBoost with normal boosting \n",
    "-\tAdaBoost with ordinal boosting \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization the normalization factor for Decision trees\n",
    "from numpy import arange\n",
    "\n",
    "def WIGR_power_finder_DT(data,kfold,min_samples_leaf):\n",
    "    print('WIGR_power_finder_DT:')\n",
    "    powers1 = {}\n",
    "    powers2 = {}\n",
    "    powers3 = {}\n",
    "    powers4 = {}\n",
    "    powers5 = {}\n",
    "    powers6 = {}\n",
    "    \n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1] #set label columns \n",
    "    model_lvl_roc_auc_avg = []\n",
    "    models_opt = []\n",
    "    lst = list(arange(0.05, 1,0.2))+ list(arange(1, 3.5,0.5)) #list(range(0,11)) + list(arange(0.05, 1, 0.15))\n",
    "    \n",
    "    for i in lst:\n",
    "        clf1 = DecisionTreeClassifier(criterion='WIGR_mode',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf)\n",
    "        clf2 = DecisionTreeClassifier(criterion='WIGR_max',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf)\n",
    "        clf3 = DecisionTreeClassifier(criterion='WIGR_min',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf)\n",
    "        clf4 = DecisionTreeClassifier(criterion='WIGR_EV',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf)\n",
    "        clf5 = DecisionTreeClassifier(criterion='WIGR_EV_fix',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf)\n",
    "        clf6 = DecisionTreeClassifier(criterion='entropy',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "      \n",
    "\n",
    "        champ_roc = []\n",
    "        for num,cl in [(6,(clf6,powers6)),(1,(clf1,powers1)),(2,(clf2,powers2)),(3,(clf3,powers3)),(4,(clf4,powers4)),(5,(clf5,powers5))]:\n",
    "                     \n",
    "            roc_auc = []\n",
    "            \n",
    "            for train_index, test_index in kfold.split(X,Y):\n",
    "                \n",
    "                x_train, x_test = X.loc[train_index,:], X.loc[test_index,:]\n",
    "                y_train, y_test = Y.loc[train_index], Y.loc[test_index]\n",
    "                \n",
    "                cl[0].fit(x_train, y_train)\n",
    "\n",
    "\n",
    "                pred = cl[0].predict(x_test)\n",
    "                #handle roc_auc predict less classes than max\n",
    "                Y_uniqe = list(pd.unique(Y))\n",
    "                Y_uniqe.sort()\n",
    "                y_test_uniqe = list(pd.unique(y_test))\n",
    "                pred_uniqe = list(pd.unique(pred))\n",
    "                y_test_pd = pd.get_dummies(y_test)\n",
    "                pred_pd = pd.get_dummies(pred)\n",
    "                y_test_pd = y_test_pd.reset_index(drop = True)\n",
    "                pred_pd = pred_pd.reset_index(drop = True)\n",
    "\n",
    "                for label in Y_uniqe:\n",
    "                    if label not in pred_uniqe:\n",
    "                        pred_pd[label] = 0\n",
    "\n",
    "\n",
    "                    if label not in y_test_uniqe:\n",
    "                        y_test_pd[label] = 0 \n",
    "                        extra_row = [0]*y_test_pd.shape[1]\n",
    "                        extra_row[-1] = 1\n",
    "                        y_test_pd.loc[len(y_test_pd)+2]= extra_row\n",
    "                        extra_row_pred = [0]*pred_pd.shape[1]\n",
    "                        extra_row_pred[-1] = 1\n",
    "                        pred_pd.loc[len(y_test_pd)-1]= extra_row_pred\n",
    "                        y_test_pd = y_test_pd.reset_index(drop = True)\n",
    "                        pred_pd = pred_pd.reset_index(drop = True)\n",
    "                        \n",
    "                roc_auc.append(metrics.roc_auc_score(y_test_pd,pred_pd,average = 'weighted' , multi_class = 'ovr'))\n",
    "\n",
    "\n",
    "            if num == 6 : \n",
    "                champ_roc = roc_auc \n",
    "        \n",
    "            if np.average(roc_auc) > np.average(champ_roc) and (stats.ttest_ind(champ_roc,roc_auc)[1] < 0.151 or stats.ttest_rel(champ_roc,roc_auc)[1]<0.151):\n",
    "                print('jackpot!!!')\n",
    "                \n",
    "                cl[1][i] = np.average(roc_auc)\n",
    "            else:\n",
    "                cl[1][i] = np.average(roc_auc) - 0.2\n",
    "        print(i)\n",
    "        \n",
    "    print  (list(zip([max(powers1.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers2.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers3.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers4.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers5.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers6.items(), key=operator.itemgetter(1))[0]],\n",
    "            [powers1[max(powers1.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers2[max(powers2.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers3[max(powers3.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers4[max(powers4.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers5[max(powers5.items(), key=operator.itemgetter(1))[0]],\n",
    "           powers6[ max(powers6.items(), key=operator.itemgetter(1))[0]]])))\n",
    "    \n",
    "    return  [max(powers1.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers2.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers3.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers4.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers5.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers6.items(), key=operator.itemgetter(1))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization the normalization factor for AdaBoost with normal boosting \n",
    " \n",
    "from numpy import arange\n",
    "\n",
    "def WIGR_power_finder_ADA(data,kfold,min_samples_leaf):\n",
    "    print('WIGR_power_finder_ADA:')\n",
    "    powers1 = {}\n",
    "    powers2 = {}\n",
    "    powers3 = {}\n",
    "    powers4 = {}\n",
    "    powers5 = {}\n",
    "    powers6 = {}\n",
    "    \n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1] #set label columns \n",
    "    model_lvl_roc_auc_avg = []\n",
    "    models_opt = []\n",
    "    lst = list(arange(0.05, 1,0.2))+ list(arange(1, 3.5,0.5)) #list(range(0,11)) + list(arange(0.05, 1, 0.15))\n",
    "    \n",
    "    for i in lst:\n",
    "\n",
    "        clf1 = AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_mode',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,max_depth = 1))\n",
    "        clf2 = AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_max',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,max_depth = 1))\n",
    "        clf3 = AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_min',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,max_depth = 1))\n",
    "        clf4 = AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_EV',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,max_depth = 1))\n",
    "        clf5 = AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_EV_fix',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,max_depth = 1))\n",
    "        clf6 = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,max_depth = 1))\n",
    "\n",
    "      \n",
    "\n",
    "        champ_roc = []\n",
    "        for num,cl in [(6,(clf6,powers6)),(1,(clf1,powers1)),(2,(clf2,powers2)),(3,(clf3,powers3)),(4,(clf4,powers4)),(5,(clf5,powers5))]:\n",
    "            \n",
    "            roc_auc = []\n",
    "            \n",
    "            for train_index, test_index in kfold.split(X,Y):\n",
    "                \n",
    "                x_train, x_test = X.loc[train_index,:], X.loc[test_index,:]\n",
    "                y_train, y_test = Y.loc[train_index], Y.loc[test_index]\n",
    "                \n",
    "                cl[0].fit(x_train, y_train)\n",
    "\n",
    "\n",
    "                pred = cl[0].predict(x_test)\n",
    "                #handle roc_auc predict less classes than max\n",
    "                Y_uniqe = list(pd.unique(Y))\n",
    "                Y_uniqe.sort()\n",
    "                y_test_uniqe = list(pd.unique(y_test))\n",
    "                pred_uniqe = list(pd.unique(pred))\n",
    "                y_test_pd = pd.get_dummies(y_test)\n",
    "                pred_pd = pd.get_dummies(pred)\n",
    "                y_test_pd = y_test_pd.reset_index(drop = True)\n",
    "                pred_pd = pred_pd.reset_index(drop = True)\n",
    "\n",
    "                for label in Y_uniqe:\n",
    "                    if label not in pred_uniqe:\n",
    "                        pred_pd[label] = 0\n",
    "\n",
    "\n",
    "                    if label not in y_test_uniqe:\n",
    "                        y_test_pd[label] = 0 \n",
    "                        extra_row = [0]*y_test_pd.shape[1]\n",
    "                        extra_row[-1] = 1\n",
    "                        y_test_pd.loc[len(y_test_pd)+2]= extra_row\n",
    "                        extra_row_pred = [0]*pred_pd.shape[1]\n",
    "                        extra_row_pred[-1] = 1\n",
    "                        pred_pd.loc[len(y_test_pd)-1]= extra_row_pred\n",
    "                        y_test_pd = y_test_pd.reset_index(drop = True)\n",
    "                        pred_pd = pred_pd.reset_index(drop = True)\n",
    "                        \n",
    "                roc_auc.append(metrics.roc_auc_score(y_test_pd,pred_pd,average = 'weighted' , multi_class = 'ovr'))\n",
    "\n",
    "         \n",
    "            if num == 6 : \n",
    "                champ_roc = roc_auc \n",
    "        \n",
    "            if np.average(roc_auc) > np.average(champ_roc) and (stats.ttest_ind(champ_roc,roc_auc)[1] < 0.151 or stats.ttest_rel(champ_roc,roc_auc)[1]<0.151):\n",
    "                print('jackpot!!!')\n",
    "                \n",
    "                cl[1][i] = np.average(roc_auc)\n",
    "            else:\n",
    "                cl[1][i] = np.average(roc_auc) - 0.2\n",
    "                \n",
    "        print(i)\n",
    "        \n",
    "    print  (list(zip([max(powers1.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers2.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers3.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers4.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers5.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers6.items(), key=operator.itemgetter(1))[0]],\n",
    "            [powers1[max(powers1.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers2[max(powers2.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers3[max(powers3.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers4[max(powers4.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers5[max(powers5.items(), key=operator.itemgetter(1))[0]],\n",
    "           powers6[ max(powers6.items(), key=operator.itemgetter(1))[0]]])))\n",
    "    \n",
    "    return  [max(powers1.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers2.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers3.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers4.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers5.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers6.items(), key=operator.itemgetter(1))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization the normalization factor for AdaBoost with ordinal boosting \n",
    "from numpy import arange\n",
    "\n",
    "def WIGR_power_finder_ADA_ordinal(data,kfold,min_samples_leaf,alg ='SAMME',Ordinal_problem =0,max_depth =1):\n",
    "    print('WIGR_power_finder_ADA:' +alg + ' ' + str(Ordinal_problem) )\n",
    "    powers1 = {}\n",
    "    powers2 = {}\n",
    "    powers3 = {}\n",
    "    powers4 = {}\n",
    "    powers5 = {}\n",
    "    powers6 = {}\n",
    "    \n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1] #set label columns \n",
    "    model_lvl_roc_auc_avg = []\n",
    "    models_opt = []\n",
    "    lst = list(arange(0.05, 1,0.2))+ list(arange(1, 3.5,0.5)) #list(range(0,11)) + list(arange(0.05, 1, 0.15))\n",
    "    \n",
    "    for i in lst:\n",
    "\n",
    "        clf1 = AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_mode',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,max_depth = max_depth),Ordinal_problem=Ordinal_problem , algorithm = alg)\n",
    "        clf2 = AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_max',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,max_depth = max_depth),Ordinal_problem=Ordinal_problem , algorithm = alg)\n",
    "        clf3 = AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_min',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,max_depth = max_depth),Ordinal_problem=Ordinal_problem , algorithm = alg)\n",
    "        clf4 = AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_EV',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,max_depth =max_depth),Ordinal_problem=Ordinal_problem , algorithm = alg)\n",
    "        clf5 = AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_EV_fix',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,max_depth = max_depth),Ordinal_problem=Ordinal_problem , algorithm = alg)\n",
    "        clf6 = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,max_depth = max_depth), algorithm = alg)\n",
    "\n",
    "        champ_roc = []\n",
    "        for num,cl in [(6,(clf6,powers6)),(1,(clf1,powers1)),(2,(clf2,powers2)),(3,(clf3,powers3)),(4,(clf4,powers4)),(5,(clf5,powers5))]:\n",
    " \n",
    "            roc_auc = []\n",
    "            \n",
    "            for train_index, test_index in kfold.split(X,Y):\n",
    "                \n",
    "                x_train, x_test = X.loc[train_index,:], X.loc[test_index,:]\n",
    "                y_train, y_test = Y.loc[train_index], Y.loc[test_index]\n",
    "                \n",
    "                cl[0].fit(x_train, y_train)\n",
    "\n",
    "\n",
    "                pred = cl[0].predict(x_test)\n",
    "                #handle roc_auc predict less classes than max\n",
    "                Y_uniqe = list(pd.unique(Y))\n",
    "                Y_uniqe.sort()\n",
    "                y_test_uniqe = list(pd.unique(y_test))\n",
    "                pred_uniqe = list(pd.unique(pred))\n",
    "                y_test_pd = pd.get_dummies(y_test)\n",
    "                pred_pd = pd.get_dummies(pred)\n",
    "                y_test_pd = y_test_pd.reset_index(drop = True)\n",
    "                pred_pd = pred_pd.reset_index(drop = True)\n",
    "\n",
    "                for label in Y_uniqe:\n",
    "                    if label not in pred_uniqe:\n",
    "                        pred_pd[label] = 0\n",
    "\n",
    "\n",
    "                    if label not in y_test_uniqe:\n",
    "                        y_test_pd[label] = 0 \n",
    "                        extra_row = [0]*y_test_pd.shape[1]\n",
    "                        extra_row[-1] = 1\n",
    "                        y_test_pd.loc[len(y_test_pd)+2]= extra_row\n",
    "                        extra_row_pred = [0]*pred_pd.shape[1]\n",
    "                        extra_row_pred[-1] = 1\n",
    "                        pred_pd.loc[len(y_test_pd)-1]= extra_row_pred\n",
    "                        y_test_pd = y_test_pd.reset_index(drop = True)\n",
    "                        pred_pd = pred_pd.reset_index(drop = True)\n",
    "                        \n",
    "                roc_auc.append(metrics.roc_auc_score(y_test_pd,pred_pd,average = 'weighted' , multi_class = 'ovr'))\n",
    "\n",
    "         \n",
    "            if num == 6 : \n",
    "                champ_roc = roc_auc \n",
    "        \n",
    "            if np.average(roc_auc) > np.average(champ_roc) and (stats.ttest_ind(champ_roc,roc_auc)[1] < 0.151 or stats.ttest_rel(champ_roc,roc_auc)[1]<0.151):\n",
    "                print('jackpot!!!')\n",
    "                \n",
    "                cl[1][i] = np.average(roc_auc)\n",
    "            else:\n",
    "                cl[1][i] = np.average(roc_auc) - 0.2\n",
    "        print(i)\n",
    "        \n",
    "    print  (list(zip([max(powers1.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers2.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers3.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers4.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers5.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers6.items(), key=operator.itemgetter(1))[0]],\n",
    "            [powers1[max(powers1.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers2[max(powers2.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers3[max(powers3.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers4[max(powers4.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers5[max(powers5.items(), key=operator.itemgetter(1))[0]],\n",
    "           powers6[ max(powers6.items(), key=operator.itemgetter(1))[0]]])))\n",
    "    \n",
    "    return  [max(powers1.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers2.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers3.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers4.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers5.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers6.items(), key=operator.itemgetter(1))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization the normalization factor for Random forest\n",
    "\n",
    "from numpy import arange\n",
    "\n",
    "def WIGR_power_finder_RF(data,kfold,min_samples_leaf,n = 100):\n",
    "    print('WIGR_power_finder_RF:')\n",
    "    powers1 = {}\n",
    "    powers2 = {}\n",
    "    powers3 = {}\n",
    "    powers4 = {}\n",
    "    powers5 = {}\n",
    "    powers6 = {}\n",
    "    \n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1] #set label columns \n",
    "    model_lvl_roc_auc_avg = []\n",
    "    models_opt = []\n",
    "    lst = list(arange(0.05, 1,0.2))+ list(arange(1, 3.5,0.5)) #list(range(0,11)) + list(arange(0.05, 1, 0.15))\n",
    "    \n",
    "    for i in lst:\n",
    "        clf1 = RandomForestClassifier(criterion='WIGR_mode',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,n_estimators=  n)\n",
    "        clf2 = RandomForestClassifier(criterion='WIGR_max',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,n_estimators=  n)\n",
    "        clf3 = RandomForestClassifier(criterion='WIGR_min',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,n_estimators= n)\n",
    "        clf4 = RandomForestClassifier(criterion='WIGR_EV',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,n_estimators = n)\n",
    "        clf5 = RandomForestClassifier(criterion='WIGR_EV_fix',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,n_estimators = n)\n",
    "        clf6 = RandomForestClassifier(criterion='entropy',random_state=42,WIGR_power= i,min_samples_leaf=min_samples_leaf,n_estimators=  n)\n",
    "        \n",
    "\n",
    "        champ_roc = []\n",
    "        for num,cl in [(6,(clf6,powers6)),(1,(clf1,powers1)),(2,(clf2,powers2)),(3,(clf3,powers3)),(4,(clf4,powers4)),(5,(clf5,powers5))]:\n",
    "             \n",
    "            roc_auc = []\n",
    "            \n",
    "            for train_index, test_index in kfold.split(X,Y):\n",
    "                \n",
    "                x_train, x_test = X.loc[train_index,:], X.loc[test_index,:]\n",
    "                y_train, y_test = Y.loc[train_index], Y.loc[test_index]\n",
    "                \n",
    "                cl[0].fit(x_train, y_train)\n",
    "\n",
    "\n",
    "                pred = cl[0].predict(x_test)\n",
    "                #handle roc_auc predict less classes than max\n",
    "                Y_uniqe = list(pd.unique(Y))\n",
    "                Y_uniqe.sort()\n",
    "                y_test_uniqe = list(pd.unique(y_test))\n",
    "                pred_uniqe = list(pd.unique(pred))\n",
    "                y_test_pd = pd.get_dummies(y_test)\n",
    "                pred_pd = pd.get_dummies(pred)\n",
    "                y_test_pd = y_test_pd.reset_index(drop = True)\n",
    "                pred_pd = pred_pd.reset_index(drop = True)\n",
    "\n",
    "                for label in Y_uniqe:\n",
    "                    if label not in pred_uniqe:\n",
    "                        pred_pd[label] = 0\n",
    "\n",
    "\n",
    "                    if label not in y_test_uniqe:\n",
    "                        y_test_pd[label] = 0 \n",
    "                        extra_row = [0]*y_test_pd.shape[1]\n",
    "                        extra_row[-1] = 1\n",
    "                        y_test_pd.loc[len(y_test_pd)+2]= extra_row\n",
    "                        extra_row_pred = [0]*pred_pd.shape[1]\n",
    "                        extra_row_pred[-1] = 1\n",
    "                        pred_pd.loc[len(y_test_pd)-1]= extra_row_pred\n",
    "                        y_test_pd = y_test_pd.reset_index(drop = True)\n",
    "                        pred_pd = pred_pd.reset_index(drop = True)\n",
    "                        \n",
    "                roc_auc.append(metrics.roc_auc_score(y_test_pd,pred_pd,average = 'weighted' , multi_class = 'ovr'))\n",
    "\n",
    "         \n",
    "            if num == 6 : \n",
    "                champ_roc = roc_auc \n",
    "        \n",
    "            if np.average(roc_auc) > np.average(champ_roc) and (stats.ttest_ind(champ_roc,roc_auc)[1] < 0.151 or stats.ttest_rel(champ_roc,roc_auc)[1]<0.151):\n",
    "                print('jackpot!!!')\n",
    "                \n",
    "                cl[1][i] = np.average(roc_auc)\n",
    "            else:\n",
    "                cl[1][i] = np.average(roc_auc) - 0.2\n",
    "        print(i)\n",
    "        \n",
    "    print  (list(zip([max(powers1.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers2.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers3.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers4.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers5.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers6.items(), key=operator.itemgetter(1))[0]],\n",
    "            [powers1[max(powers1.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers2[max(powers2.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers3[max(powers3.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers4[max(powers4.items(), key=operator.itemgetter(1))[0]],\n",
    "            powers5[max(powers5.items(), key=operator.itemgetter(1))[0]],\n",
    "           powers6[ max(powers6.items(), key=operator.itemgetter(1))[0]]])))\n",
    "    \n",
    "    return  [max(powers1.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers2.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers3.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers4.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers5.items(), key=operator.itemgetter(1))[0],\n",
    "            max(powers6.items(), key=operator.itemgetter(1))[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and create all model for the experiment.\n",
    "This function create all models object that will be run during our experiments \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tuple contain: (model_index, model_name, model object).    \n",
    "For the counterpart index will be len== 1.   \n",
    "For our ordinal model the index will be len==2 and the first digit equal to the counterpart \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def opt_models(data,kfold,min_samples_leaf):\n",
    "    # Run the optimization function the normalization factor  \n",
    "    power_wigr_DT = WIGR_power_finder_DT(data,kfold,min_samples_leaf)\n",
    "    power_wigr_RF = WIGR_power_finder_RF(data,kfold,min_samples_leaf)\n",
    "    power_wigr_ADA_SAMME = WIGR_power_finder_ADA_ordinal(data,kfold,min_samples_leaf,alg = 'SAMME')\n",
    "    power_wigr_ADA_ordinal = WIGR_power_finder_ADA_ordinal(data,kfold,min_samples_leaf,Ordinal_problem = 1)\n",
    "\n",
    "    models = []\n",
    "\n",
    "    #Insert model to be compared and analyzed \n",
    "    models.append(('999',\"Logistic Regression:\",LogisticRegression(max_iter= 3)))\n",
    "    models.append(('999',\"Naive Bayes:\",GaussianNB()))\n",
    "    models.append(('999',\"K-Nearest Neighbour:\",KNeighborsClassifier(n_neighbors=3)))\n",
    "    models.append(('999',\"eXtreme Gradient Boost G:\",XGBClassifier(min_child_weight  = min_samples_leaf )))\n",
    "    models.append(('999',\"GradientBoostingClassifier G:\",GradientBoostingClassifier(min_samples_leaf=min_samples_leaf)))\n",
    "    models.append(('999','OrdinalModel',OrdinalModel()))\n",
    "\n",
    "    models.append(('1',\"Decision Tree:\",DecisionTreeClassifier(random_state = 42,criterion='entropy',min_samples_leaf=min_samples_leaf)))\n",
    "    models.append(('10',\"WIGR_mode Decision Tree\",DecisionTreeClassifier(criterion='WIGR_mode',random_state=42,WIGR_power= power_wigr_DT[0],min_samples_leaf=min_samples_leaf)))\n",
    "    models.append(('10',\"WIGR_max Decision Tree\",DecisionTreeClassifier(criterion='WIGR_max',random_state=42,WIGR_power= power_wigr_DT[1],min_samples_leaf=min_samples_leaf)))\n",
    "    models.append(('10',\"WIGR_min Decision Tree\",DecisionTreeClassifier(criterion='WIGR_min',random_state=42,WIGR_power= power_wigr_DT[2],min_samples_leaf=min_samples_leaf)))\n",
    "    models.append(('10',\"WIGR_EV Decision Tree\",DecisionTreeClassifier(criterion='WIGR_EV',random_state=42,WIGR_power= power_wigr_DT[3],min_samples_leaf=min_samples_leaf)))\n",
    "    models.append(('10',\"WIGR_EV_fix Decision Tree\",DecisionTreeClassifier(criterion='WIGR_EV_fix',random_state=42,WIGR_power= power_wigr_DT[4],min_samples_leaf=min_samples_leaf)))\n",
    "\n",
    "    models.append(('3',\"Random Forest:\",RandomForestClassifier(criterion='entropy',random_state=42,min_samples_leaf=min_samples_leaf)))\n",
    "    models.append(('30',\"WIGR_mode Random Forest\",RandomForestClassifier(criterion='WIGR_mode',random_state=42,WIGR_power= power_wigr_RF[0],min_samples_leaf=min_samples_leaf)))\n",
    "    models.append(('30',\"WIGR_max Random Forest\",RandomForestClassifier(criterion='WIGR_max',random_state=42,WIGR_power= power_wigr_RF[1],min_samples_leaf=min_samples_leaf)))\n",
    "    models.append(('30',\"WIGR_min Random Forest\",RandomForestClassifier(criterion='WIGR_min',random_state=42,WIGR_power= power_wigr_RF[2],min_samples_leaf=min_samples_leaf)))\n",
    "    models.append(('30',\"WIGR_EV Random Forest\",RandomForestClassifier(criterion='WIGR_EV',random_state=42,WIGR_power= power_wigr_RF[3],min_samples_leaf=min_samples_leaf)))\n",
    "    models.append(('30',\"WIGR_EV_fix Random Forest\",RandomForestClassifier(criterion='WIGR_EV_fix',random_state=42,WIGR_power= power_wigr_RF[4],min_samples_leaf=min_samples_leaf)))\n",
    "\n",
    "    models.append(('4',\"AdaBoostClassifier SAMME\",AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy',random_state=1),algorithm='SAMME')))\n",
    "    models.append(('40',\"WIGR_mode AdaBoostClassifier SAMME\",AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_mode',random_state=42,WIGR_power= power_wigr_ADA_SAMME[0]),algorithm='SAMME')))\n",
    "    models.append(('40',\"WIGR_max AdaBoostClassifier SAMME\",AdaBoostClassifier(DecisionTreeClassifier (criterion='WIGR_max',random_state=42,WIGR_power=power_wigr_ADA_SAMME[1]),algorithm='SAMME')))\n",
    "    models.append(('40',\"WIGR_min AdaBoostClassifier SAMME\",AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_min',random_state=42,WIGR_power= power_wigr_ADA_SAMME[2]),algorithm='SAMME')))\n",
    "    models.append(('40',\"WIGR_EV AdaBoostClassifier SAMME\",AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_EV',random_state=42,WIGR_power= power_wigr_ADA_SAMME[3]),algorithm='SAMME')))\n",
    "    models.append(('40',\"WIGR_EV_fix AdaBoostClassifier SAMME\",AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_EV_fix',random_state=42,WIGR_power= power_wigr_ADA_SAMME[4]),algorithm='SAMME')))\n",
    "\n",
    "    models.append(('5',\"AdaBoostClassifier SAMME\",AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy',random_state=1),algorithm='SAMME')))\n",
    "    models.append(('50',\"WIGR_mode AdaBoostClassifier Ordinal\",AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_mode',random_state=42,WIGR_power= power_wigr_ADA_ordinal[0]),Ordinal_problem=1)))\n",
    "    models.append(('50',\"WIGR_max AdaBoostClassifier Ordinal\",AdaBoostClassifier(DecisionTreeClassifier (criterion='WIGR_max',random_state=42,WIGR_power=power_wigr_ADA_ordinal[1]),Ordinal_problem=1)))\n",
    "    models.append(('50',\"WIGR_min AdaBoostClassifier Ordinal\",AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_min',random_state=42,WIGR_power= power_wigr_ADA_ordinal[2]),Ordinal_problem=1)))\n",
    "    models.append(('50',\"WIGR_EV AdaBoostClassifier Ordinal\",AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_EV',random_state=42,WIGR_power= power_wigr_ADA_ordinal[3]),Ordinal_problem=1)))\n",
    "    models.append(('50',\"WIGR_EV_fix AdaBoostClassifier Ordinal\",AdaBoostClassifier(DecisionTreeClassifier(criterion='WIGR_EV_fix',random_state=42,WIGR_power= pow.ar_wigr_ADA_ordinal[4]),Ordinal_problem=1)))\n",
    "\n",
    "    models.append(('6',\"AdaBoostClassifier SAMME\",AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy',random_state=1,max_depth=1),algorithm='SAMME')))\n",
    "    models.append(('60',\"AdaBoostClassifier boost Ordinal\",AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy',random_state=42),Ordinal_problem=1)))\n",
    "\n",
    "    models.append(('7',\"AdaBoostClassifier SAMME2\",AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy',random_state=1,max_depth=1),algorithm='SAMME')))\n",
    "    models.append(('70',\"AdaBoostClassifier boost Ordinal2\",AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy',random_state=1,max_depth=1),Ordinal_problem=1)))\n",
    "    \n",
    "    \n",
    "    #Ensemble experiment models   \n",
    "#     permut = [\n",
    "#         (\"WIGR_mode Random Forest\",RandomForestClassifier(criterion='WIGR_mode',random_state=42,WIGR_power= power_wigr_RF[0],min_samples_leaf=min_samples_leaf)),\n",
    "#         (\"WIGR_max Random Forest\",RandomForestClassifier(criterion='WIGR_max',random_state=42,WIGR_power= power_wigr_RF[1],min_samples_leaf=min_samples_leaf)),\n",
    "#         (\"WIGR_min Random Forest\",RandomForestClassifier(criterion='WIGR_min',random_state=42,WIGR_power= power_wigr_RF[2],min_samples_leaf=min_samples_leaf)),\n",
    "#         (\"eXtreme Gradient Boost:\",XGBClassifier(min_child_weight  = min_samples_leaf )),\n",
    "#         (\"GradientBoostingClassifier:\",GradientBoostingClassifier(min_samples_leaf=min_samples_leaf))\n",
    "#     ]\n",
    "\n",
    "\n",
    "#     models.append(('1',\"Logistic Regression:\",LogisticRegression(max_iter= 3)))\n",
    "#     i=11\n",
    "#     vote = VotingClassifier(estimators=[permut[0]]+[permut[1]]+[permut[2]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[0][0]) + str(permut[1][0]) + str(permut[2][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[3]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[3][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[4]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[4][0]) +'hard',vote))\n",
    "\n",
    "#     models.append(('2',\"eXtreme Gradient Boost:\",XGBClassifier(min_child_weight  = min_samples_leaf,max_depth =1 )))\n",
    "#     i=22\n",
    "#     vote = VotingClassifier(estimators=[permut[0]]+[permut[1]]+[permut[2]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[0][0]) + str(permut[1][0]) + str(permut[2][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[3]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[3][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[4]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[4][0]) +'hard',vote))\n",
    "\n",
    "#     models.append(('3',\"GradientBoostingClassifier:\",GradientBoostingClassifier(min_samples_leaf=min_samples_leaf,n_estimators =8)))\n",
    "#     i=33\n",
    "#     vote = VotingClassifier(estimators=[permut[0]]+[permut[1]]+[permut[2]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[0][0]) + str(permut[1][0]) + str(permut[2][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[3]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[3][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[4]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[4][0]) +'hard',vote))\n",
    "\n",
    "\n",
    "#     models.append(('4','OrdinalModel',OrdinalModel()))\n",
    "#     i=44\n",
    "#     vote = VotingClassifier(estimators=[permut[0]]+[permut[1]]+[permut[2]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[0][0]) + str(permut[1][0]) + str(permut[2][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[3]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[3][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[4]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[4][0]) +'hard',vote))\n",
    "\n",
    "#     models.append(('5',str(permut[0][0]),permut[0][1]))\n",
    "#     i=55\n",
    "#     vote = VotingClassifier(estimators=[permut[0]]+[permut[1]]+[permut[2]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[0][0]) + str(permut[1][0]) + str(permut[2][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[3]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[3][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[4]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[4][0]) +'hard',vote))\n",
    "    \n",
    "#     models.append(('6',str(permut[1][0]),permut[1][1]))\n",
    "#     i=66\n",
    "#     vote = VotingClassifier(estimators=[permut[0]]+[permut[1]]+[permut[2]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[0][0]) + str(permut[1][0]) + str(permut[2][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[3]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[3][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[4]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[4][0]) +'hard',vote))\n",
    "\n",
    "#     models.append(('7',str(permut[2][0]),permut[2][1]))\n",
    "#     i=77\n",
    "#     vote = VotingClassifier(estimators=[permut[0]]+[permut[1]]+[permut[2]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[0][0]) + str(permut[1][0]) + str(permut[2][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[3]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[3][0]) +'hard',vote))\n",
    "#     vote = VotingClassifier(estimators=[permut[2]]+[permut[1]]+[permut[4]],voting='hard')\n",
    "#     models.append((str(i),str(i)+'_VotingClassifier_'+ str(permut[2][0]) + str(permut[1][0]) + str(permut[4][0]) +'hard',vote))\n",
    "\n",
    "\n",
    "    print('Models appended...')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function run the experiment on all models per dataset and return the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_all(file,kfold,min_split):\n",
    "    \n",
    "    model_df = pd.DataFrame([])\n",
    "    class_df = pd.DataFrame([])\n",
    "    data_src = file\n",
    "    print(data_src)\n",
    "    data = pd.read_csv(file,header=None)\n",
    "    rd =180\n",
    "    sp = 5\n",
    "    kfold = StratifiedKFold(n_splits=sp,shuffle = True,random_state = rd)\n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1] #set label columns \n",
    "    print(\"number of class : \",len(pd.unique(Y)))\n",
    "    models = opt_models (data,kfold,min_split)\n",
    "    a,b,c,d= calc_results(models,data,kfold,rd,sp)\n",
    "    \n",
    "    for dfs in [a,b,c,d]:\n",
    "        dfs['file'] = data_src.replace('.csv','')\n",
    "        dfs['type'] = file.split('/')[1]\n",
    "        if file.split('/')[1] != 'Orignial': \n",
    "            number_of_class = file.split('.')[0][-1]\n",
    "            file_name = file.split('/')[-1][:-6]\n",
    "        else:\n",
    "            number_of_class = 3\n",
    "            file_name = file.split('/')[-1][:-4]\n",
    "        \n",
    "        dfs['class'] = len(pd.unique(Y))\n",
    "        dfs['min_split'] = min_split\n",
    "        dfs['data'] = file_name\n",
    "    model_df= pd.concat([model_df,a,b])\n",
    "    class_df=pd.concat([class_df,c,d])\n",
    "    print('----------------------------------------')\n",
    "    model_df = model_df.reset_index()\n",
    "    class_df = class_df.reset_index()\n",
    "    return model_df,class_df,models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kfold object \n",
    "kfold = StratifiedKFold(n_splits=20,shuffle = True,random_state = 69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This function run the experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(path,kfold,min_split ):\n",
    "    model_df = pd.DataFrame([])\n",
    "    class_df = pd.DataFrame([])\n",
    "    for path in paths:\n",
    "    \n",
    "        for data_src in os.listdir(path):\n",
    "            for i in [10,15]:# range(10,11,10):\n",
    "                \n",
    "                if data_src=='.DS_Store':\n",
    "                    continue\n",
    "                model_df_curr,class_df_crr,models =  run_one_all(path + data_src,kfold,i)\n",
    "                model_df= pd.concat([model_df,model_df_curr])\n",
    "                class_df= pd.concat([class_df,class_df_crr])\n",
    "\n",
    "    #            print('----------------------------------------')\n",
    "            \n",
    "    model_df = model_df.reset_index(drop = True)\n",
    "    class_df = class_df.reset_index(drop = True)\n",
    "    \n",
    "    for x,m,n in models:\n",
    "        model_df[m] = model_df[m].astype('object')\n",
    "        \n",
    "    return model_df,class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "  0% |                                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/pyrim_3.csv\n",
      "number of class :  3\n",
      "Models appended...\n",
      "total models :  1\n",
      "3/pyrim_3.csv\n",
      "3/pyrim_3.csv\n",
      "3/pyrim_3.csv\n",
      "3/pyrim_3.csv\n",
      "----------------------------------------\n",
      "3/pyrim_3.csv\n",
      "number of class :  3\n",
      "Models appended...\n",
      "total models :  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\n",
      "  0% |                                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/pyrim_3.csv\n",
      "3/pyrim_3.csv\n",
      "3/pyrim_3.csv\n",
      "3/pyrim_3.csv\n",
      "----------------------------------------\n",
      "3/stock_3.csv\n",
      "number of class :  3\n",
      "Models appended...\n",
      "total models :  1\n",
      "3/stock_3.csv\n",
      "3/stock_3.csv\n",
      "3/stock_3.csv\n",
      "3/stock_3.csv\n",
      "----------------------------------------\n",
      "3/stock_3.csv\n",
      "number of class :  3\n",
      "Models appended...\n",
      "total models :  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/stock_3.csv\n",
      "3/stock_3.csv\n",
      "3/stock_3.csv\n",
      "3/stock_3.csv\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "paths = [r'folder_name/'] # in which folder our data is kept \n",
    "results_per_model,results_per_class = run_all(paths,kfold,10)\n",
    "results_per_model.to_excel('results_per_model.xlsx',index=False)\n",
    "results_per_class.to_excel('results_per_class.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
